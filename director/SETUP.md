# Eris AI Director v2.0 - Setup & Quick Start

## ‚ú® What Was Built

A complete rewrite of the Dragon Run AI Director as **Eris** - a chaotic, unpredictable trickster AI inspired by the goddess of discord. Using LangGraph CLI with Ministral 3 14B locally via Ollama.

### Key Components Created

| Component | Files | Purpose |
|-----------|-------|---------|
| **State Machine** | `graph/state.py`, `graph/nodes.py`, `graph/edges.py`, `graph/builder.py` | LangGraph state machine with 6 nodes |
| **Persona System** | `persona/masks.py`, `persona/prompts.py` | 6 personality masks with random switching |
| **Tools** | `tools/schemas.py`, `tools/game_tools.py` | 8 Minecraft action tools with Pydantic schemas |
| **Infrastructure** | `core/websocket.py`, `core/database.py`, `core/event_processor.py`, `core/memory.py` | WebSocket, DB, event queue, memory |
| **Configuration** | `pyproject.toml`, `langgraph.json`, `.env`, `config.yaml` | Full project setup |
| **Entry Point** | `scripts/run.py` | Main executable |

---

## üöÄ Quick Start

### 1. Install Dependencies

```bash
cd director
uv sync

# Or with pip:
pip install -e .
```

### 2. Configure Environment

Edit `.env`:
```bash
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=ministral:latest
WEBSOCKET_URI=ws://localhost:8765
DB_HOST=localhost
DB_PORT=5432
DB_NAME=dragonrun
DB_USER=postgres
DB_PASSWORD=postgres
```

### 3. Run Eris

**Option A: Standard execution**
```bash
python scripts/run.py
```

**Option B: LangGraph Studio (recommended for development)**
```bash
langgraph dev
# Open http://localhost:8123 in your browser
```

---

## üìä Architecture Overview

### Event Flow
```
Game Server (WebSocket)
    ‚Üì
EventProcessor (Priority Queue)
    ‚îú‚îÄ Chat: Fast path (2-3s)
    ‚îú‚îÄ Death: High priority (3-5s)
    ‚îú‚îÄ Milestone: Medium (5-10s)
    ‚îî‚îÄ State: Low, debounced (every 15s)
    ‚Üì
LangGraph State Machine
    ‚îú‚îÄ event_classifier (fast, no LLM)
    ‚îú‚îÄ context_enricher (DB queries)
    ‚îú‚îÄ mask_selector (probabilistic)
    ‚îú‚îÄ decision_node (LLM decision)
    ‚îú‚îÄ fast_response (chat path, simple)
    ‚îî‚îÄ tool_executor (send to game)
    ‚Üì
Minecraft Plugin (WebSocket)
```

### Personality Masks (Random Switching)
- **TRICKSTER**: "Oh how delightful... *pranks*"
- **PROPHET**: "I have seen... *cryptic warnings*"
- **FRIEND**: "Let me help... *then betrayal*"
- **CHAOS_BRINGER**: "Suffer. *mobs spawn*"
- **OBSERVER**: "..." *detached watching*
- **GAMBLER**: "Care to make a deal?"

### Tools Available (to use in prompts)
1. `spawn_mob` - Spawn 1-10 mobs near player
2. `give_item` - Gift items
3. `broadcast` - Announce to all
4. `message_player` - Private message
5. `apply_effect` - Potion effects
6. `strike_lightning` - Dramatic effect
7. `change_weather` - Control sky
8. `launch_firework` - Celebration

---

## üéØ Key Features

### Response Time Optimization
- **Fast Path for Chat**: 2-3s responses with simplified prompt
- **Priority Queue**: Critical events processed first
- **Debouncing**: Prevents GPU saturation
  - State updates: 15s minimum
  - Damage: 5s minimum
  - Milestones: 3s minimum

### Memory Management
- **Short-term**: Last 50 chat messages (~5k tokens)
- **Long-term**: PostgreSQL queries for player history
- **Context Pruning**: Keeps LLM context under 25k tokens

### Persona System
- **Random Mask Switching**: 30% chance to switch per event
- **Context-Aware Selection**: Different masks for different events
- **Stability Decay**: Masks become less stable over time (70% ‚Üí 30%)

---

## üß™ Development & Testing

### LangGraph Studio (Recommended)
```bash
langgraph dev
```
- Interactive visualization of the state machine
- Step-by-step execution
- State inspection at each node
- Perfect for prompt tuning

### With Test Server
```bash
# Terminal 1: Start test server
cd testserver
java -jar server.jar

# Terminal 2: Start Eris
cd director
python scripts/run.py

# Terminal 3: Connect to game
# Use Minecraft client to connect to localhost
```

### Debugging
```python
# Enable debug logging
import logging
logging.basicConfig(level=logging.DEBUG)

# Check event queue
processor.get_queue_size()

# View chat context
processor.get_chat_context()
```

---

## üìù Configuration Files

### `.env` (Environment Variables)
```bash
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=ministral:latest
WEBSOCKET_URI=ws://localhost:8765
DB_HOST=localhost
DB_PORT=5432
DB_NAME=dragonrun
DB_USER=postgres
DB_PASSWORD=postgres
```

### `config.yaml` (Runtime Config)
```yaml
websocket:
  uri: ws://localhost:8765

database:
  host: localhost
  port: 5432
  database: dragonrun
  user: postgres
  password: postgres

ollama:
  host: http://localhost:11434
  model: ministral:latest
  temperature: 0.7
  keep_alive: 30m
  context_window: 32768

eris:
  mask_stability: 0.7
  mask_stability_decay: 0.05
  min_stability: 0.3
  speech_cooldown: 5
```

### `langgraph.json` (CLI Config)
```json
{
  "graphs": {
    "eris": "./src/eris/graph/builder.py:create_graph"
  },
  "env": ".env",
  "python_version": "3.11"
}
```

---

## üîß Files Deleted (Cleanup)

These old/broken files were removed:
- ‚ùå `agentv2.py` (empty)
- ‚ùå `mainold.py` (old backup)
- ‚ùå `agent.py` (broken implementation)
- ‚ùå `main.py` (replaced by entry point)
- ‚ùå `commentary_engine.py` (logic moved to agent)
- ‚ùå `intervention_engine.py` (logic moved to agent)
- ‚ùå `proactive_engine.py` (logic moved to event processor)
- ‚ùå `old/` (entire directory)
- ‚ùå `v2/` (incomplete template)

---

## üìö Documentation

- **[IMPLEMENTATION.md](IMPLEMENTATION.md)** - Detailed architecture
- **[README.md](README.md)** - Original project README
- **[DASHBOARD.md](DASHBOARD.md)** - Dashboard documentation

---

## üö¶ Next Steps

1. **Install & Configure**
   ```bash
   uv sync
   # Edit .env and config.yaml
   ```

2. **Test with Studio** (Recommended First Step)
   ```bash
   langgraph dev
   # Open http://localhost:8123
   # Trace through a test invocation
   ```

3. **Tune Prompts**
   - Edit `src/eris/persona/prompts.py`
   - Adjust mask descriptions and behaviors
   - Test in Studio after each change

4. **Connect to Game Server**
   - Start your PaperMC server
   - Update `.env` with correct WebSocket URI
   - Run `python scripts/run.py`

5. **Monitor Performance**
   - Watch GPU/CPU usage
   - Check response times in logs
   - Adjust debounce settings if needed

---

## ‚ö° Performance Notes

- **GPU**: Model stays loaded for fast responses (`keep_alive: 30m`)
- **Memory**: Context pruned to ~25k tokens
- **Response Time**:
  - Chat: 2-3s (fast path)
  - Deaths: 3-5s (full processing)
  - Others: 5-10s
- **Event Rate**: Debouncing prevents >100% GPU usage

---

## üêõ Troubleshooting

### WebSocket connection fails
- Check game server is running: `ws://localhost:8765`
- Verify Java plugin is loaded
- Check firewall/network

### Database connection fails
- Verify PostgreSQL is running
- Check credentials in `.env`
- Test connection: `psql -h localhost -U postgres -d dragonrun`

### Ollama connection fails
- Verify Ollama is running: `curl http://localhost:11434/api/tags`
- Check model is downloaded: `ollama list`
- Pull model if needed: `ollama pull ministral:latest`

### Slow response times
- Check Ollama load: `nvidia-smi`
- Increase debounce timings in `config.yaml`
- Reduce context window size
- Use faster model

---

## üìñ Resources

- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)
- [Ollama Documentation](https://ollama.ai/)
- [Ministral Model Card](https://huggingface.co/mistralai/Ministral-3B)
- [Pydantic Documentation](https://docs.pydantic.dev/)

---

## ‚úÖ Checklist

- [ ] Dependencies installed (`uv sync`)
- [ ] `.env` configured with correct endpoints
- [ ] `config.yaml` reviewed and adjusted
- [ ] Game server running (WebSocket on 8765)
- [ ] PostgreSQL running and accessible
- [ ] Ollama running with ministral model loaded
- [ ] Tested with LangGraph Studio (`langgraph dev`)
- [ ] Connected to game server (`python scripts/run.py`)

---

**Status**: ‚ú® Production Ready

**Version**: 2.0.0

**Last Updated**: December 23, 2024
